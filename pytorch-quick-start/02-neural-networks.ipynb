{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02-neural-networks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO6rW4weLQi2XNm+aeG5nbd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ewnapQtBDfS5","colab_type":"text"},"source":["Neural networks can be constructed using the ``torch.nn`` package.\n","\n","An ``nn.Module`` contains layers, and a method ``forward(input)`` that returns the ``output``.\n","\n","A typical training procedure for a neural network is as follows:\n","\n","- Define the neural network that has some learnable parameters (or\n","  weights)\n","- Iterate over a dataset of inputs\n","- Process input through the network\n","- Compute the loss (how far is the output from being correct)\n","- Propagate gradients back into the networkâ€™s parameters\n","- Update the weights of the network, typically using a simple update rule:\n","  ``weight = weight - learning_rate * gradient``\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HEm5jFeuEBDj","colab_type":"text"},"source":["## Define the network"]},{"cell_type":"code","metadata":{"id":"R3ZOqaxmEdK0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599727424178,"user_tz":-120,"elapsed":3207,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"96r77XMVEknb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599727424178,"user_tz":-120,"elapsed":3203,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        # 1 input image channel, 6 output channels, 3x3 convolution kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        # 6 input image channel, 16 output channels, 3x3 convolution kernel\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","\n","        # Fully connected layer\n","        # affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120) # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","\n","        # Convolution operation\n","        x = self.conv1(x)\n","        # ReLU\n","        x = F.relu(x)\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(x, (2, 2))\n","\n","        # If the size is a square, we can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n","\n","        # FC layer\n","        x = x.view(-1, self.num_flat_features(x)) # flatten\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:] # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nvea7x-RHrT2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1599727424179,"user_tz":-120,"elapsed":3198,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"5b6c37ee-3b7d-40d2-afa9-023905c1e0f9"},"source":["net = Net()\n","print(net)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TM38Jfn0Hv8V","colab_type":"text"},"source":["We just have to define the ``forward`` function, and the ``backward``\n","function (where gradients are computed) is automatically defined for us\n","using ``autograd``.\n","we can use any of the Tensor operations in the ``forward`` function.\n","\n","The learnable parameters of a model are returned by ``net.parameters()``\n","\n"]},{"cell_type":"code","metadata":{"id":"jvL66m75IO6Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"error","timestamp":1599727751284,"user_tz":-120,"elapsed":1742,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"739b20bc-eff7-44b1-9c11-ea6e46e667dd"},"source":["params = list(net.parameters())"],"execution_count":17,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-15f7e3ff416a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"]}]},{"cell_type":"code","metadata":{"id":"QS9K5G4hIdUN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599727890407,"user_tz":-120,"elapsed":547,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"492a543b-e3b9-4864-daa4-934facf75958"},"source":["print(f\"#conv1's weight: {params[0].size()}\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["#conv1's weight: torch.Size([6, 1, 3, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wDodN59xIwtx","colab_type":"text"},"source":["Let's try a random 32x32 input (expected size of LeNet)"]},{"cell_type":"code","metadata":{"id":"H_RyiTXPJfmq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599728103674,"user_tz":-120,"elapsed":589,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"835b9825-d2fc-4e47-8dd6-9fdd8cbe191c"},"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0114,  0.1167, -0.0449, -0.0072, -0.0791, -0.0805, -0.0467,  0.0667,\n","         -0.0750,  0.0985]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3gNbw2QzJwtd","colab_type":"text"},"source":["Zero the gradient buffers of all parameters and backprops with random gradients:"]},{"cell_type":"code","metadata":{"id":"EHzWEUTZJ0hn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599727424181,"user_tz":-120,"elapsed":3180,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGKAB5NrJ4AC","colab_type":"text"},"source":["Note that `torch,nn` ONLY supports mini-batches. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and NOT a single sample.\n","\n","For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n","\n","For a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension."]},{"cell_type":"markdown","metadata":{"id":"dHloyBoyKbQC","colab_type":"text"},"source":["## Loss function\n","\n","A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"phkBZGDJMuAX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599727424182,"user_tz":-120,"elapsed":3175,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"9668e1d0-3480-4830-e958-60deb4b7e07d"},"source":["output = net(input)\n","target = torch.randn(10) # dummy target \n","target = target.view(1, -1) # make it the same shape as output\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","loss"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.1491, grad_fn=<MseLossBackward>)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"R07T_EA_NIQe","colab_type":"text"},"source":["If we follow `loss` in the backward direction, using its `.grad_fn` attribute, we will see a computational graph looks like this:\n","\n","```\n","input -> conv2d -> relu -> maxpool2d \n","      -> conv2d -> relu -> maxpool2d\n","      -> view -> linear -> relu -> linear -> relu -> linear\n","      -> MSELoss\n","      -> loss\n","```\n","\n","So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.\n","\n","Let's follow a few steps backward:"]},{"cell_type":"code","metadata":{"id":"k5DCXvvFFx3T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599727424182,"user_tz":-120,"elapsed":3169,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"94cb749b-7572-4e0b-bc01-7ebdcf1306b5"},"source":["print(loss.grad_fn)  # MSELoss\n","print(loss.grad_fn.next_functions[0][0])  # Linear\n","print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"],"execution_count":13,"outputs":[{"output_type":"stream","text":["<MseLossBackward object at 0x7fc3610a1dd8>\n","<AddmmBackward object at 0x7fc3610a1c88>\n","<AccumulateGrad object at 0x7fc3610a1dd8>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QbZ7TLEwN6S1","colab_type":"text"},"source":["## Backprop\n","\n","To backpropagate the error all we have to do is to `loss.backward()`. We need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n","\n","Now we call `loss.backward()`, and have a look at conv1â€™s bias gradients before and after the backward."]},{"cell_type":"code","metadata":{"id":"n_-dxcJOOaRz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599727424183,"user_tz":-120,"elapsed":3165,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"1e050ebe-24a4-4cb0-e7a3-5539567462c0"},"source":["net.zero_grad() # zeroes the gradient buffers of all parameters\n","\n","print('conv1.bias.grad before backward:')\n","print(net.conv1.bias.grad)\n","\n","loss.backward(retain_graph=True)\n","\n","print('conv1.bias.grad after backward:')\n","print(net.conv1.bias.grad)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["conv1.bias.grad before backward:\n","tensor([0., 0., 0., 0., 0., 0.])\n","conv1.bias.grad after backward:\n","tensor([-0.0037,  0.0082,  0.0034, -0.0042,  0.0045, -0.0054])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vIp23IQcOirq","colab_type":"text"},"source":["## Update weights\n","\n","The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n","```\n","weight = weight - learning_rate * gradient\n","```\n","\n","There're various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. Package `torch.optim` implements all these methods.\n","\n","Using it is quiet simple:"]},{"cell_type":"code","metadata":{"id":"bs_N_gkUPLcU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599727424183,"user_tz":-120,"elapsed":3159,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["import torch.optim as optim\n","\n","# Specify optimizer\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# In training loop:\n","# 1. Zero the gradient buffer\n","optimizer.zero_grad()\n","# 2. Forward propagation throught the neural network and get output\n","output = net(input)\n","# 3. Compute loss\n","loss = criterion(output, target)\n","# 4. Backprop loss and get loss w.r.t each parameter\n","loss.backward()\n","# 5. Update parameters \n","optimizer.step()"],"execution_count":15,"outputs":[]}]}
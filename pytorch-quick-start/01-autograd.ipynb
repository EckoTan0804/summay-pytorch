{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4KwB4gkDL1mSQvhl3CaNz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Y6YKy2d9GrNp","colab_type":"text"},"source":["The `autograd` package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."]},{"cell_type":"markdown","metadata":{"id":"G7HOHJROG0zn","colab_type":"text"},"source":["## Tensor\n","\n","`torch.Tensor` is the central class of the package. \n","\n","If we require gradient tracking\n","- Set its attribute `.requires_grad` as `True`, it starts to track all operations on it. \n","- When you finish your computation you can call `.backward()` and have all the gradients computed automatically. \n","- The gradient for this tensor will be accumulated into `.grad` attribute.\n","\n","If we don't require gradient tracking\n","- To stop a tensor from tracking history, call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n","- To prevent tracking history (and using memory), we can also wrap the code block in with `torch.no_grad():`. \n","(This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n",")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q9VQA2xj12eT","colab_type":"text"},"source":["Another important class for autograd implementation is `Function`.\n","\n","`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. \n","\n","- Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor` (except for Tensors created by the user - their `grad_fn` is None).\n","\n","- If we want to compute the derivatives, you can call `.backward()` on a Tensor. \n","\n","    - If Tensor is a scalar (i.e. it holds a one \n","element data), we don’t need to specify any arguments to `backward()`\n","    - If it has more elements, we need to specify a gradient argument that is a tensor of matching shape."]},{"cell_type":"markdown","metadata":{"id":"tQ9_lzJYg0zH","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"LRvYynW02aty","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599681416392,"user_tz":-120,"elapsed":3857,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["import torch"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bnue4B3x2dve","colab_type":"text"},"source":["Create a tensor and set `requires_grad=True` to track computation with it"]},{"cell_type":"code","metadata":{"id":"UdfQvrRk2jEC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599681416392,"user_tz":-120,"elapsed":3852,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"d77cb8b0-73e3-4b3a-8e04-70600a45a731"},"source":["x = torch.ones(2, 2, requires_grad=True)\n","x"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"PQymOxl02lyu","colab_type":"text"},"source":["Do a simple tensor operation:"]},{"cell_type":"code","metadata":{"id":"19KFqMzA3Ivp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599681416393,"user_tz":-120,"elapsed":3846,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"81cb05df-fc95-4930-c4ed-09ab0e239dd0"},"source":["y = x + 2\n","y # y was created as a result of an operation, so it has grad_fn"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3., 3.],\n","        [3., 3.]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"nFIW-lRB3J73","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599681416394,"user_tz":-120,"elapsed":3840,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"b8dc976f-2380-4388-f24d-1661a9628c3e"},"source":["y.grad_fn"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7f737edfc6d8>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"NYaWcBOf3Zcv","colab_type":"text"},"source":["Do more operations on `y`:"]},{"cell_type":"code","metadata":{"id":"4CfVJZdN3gC5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599681416394,"user_tz":-120,"elapsed":3834,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["z = y * y * 3\n","out = z.mean()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdjXUawc3re5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599681416395,"user_tz":-120,"elapsed":3830,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"af2c6400-ee88-44e8-9a92-dda3118080f6"},"source":["z"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[27., 27.],\n","        [27., 27.]], grad_fn=<MulBackward0>)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"-25pNGPi36o1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599681416395,"user_tz":-120,"elapsed":3824,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"df136ad6-bcd8-4265-d6b4-646479a25533"},"source":["out"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(27., grad_fn=<MeanBackward0>)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"_pugiX9w3_SX","colab_type":"text"},"source":["`.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag **in-place**. The input flag defaults to `False` if not given."]},{"cell_type":"code","metadata":{"id":"PYjdzaLv4MlA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599681416396,"user_tz":-120,"elapsed":3817,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"fa3bc23f-faba-4a73-8e7f-e3c58c06b7a1"},"source":["a = torch.randn(2, 2)\n","a = ((a * 3) / (a - 1))\n","print(a.requires_grad) # should be False\n","\n","# Now we set requires_grad to True explicitly\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","\n","b = a * a\n","print(b.grad_fn)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["False\n","True\n","<MulBackward0 object at 0x7f73346c0e80>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uwyPdTvi43zL","colab_type":"text"},"source":["## Gradients\n","\n","Consider a simple computation mentioned above: "]},{"cell_type":"code","metadata":{"id":"mvZy7yfI5SEY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599681416396,"user_tz":-120,"elapsed":3811,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["x = torch.ones(2, 2, requires_grad=True)\n","y = x + 2\n","z = y * y * 3\n","out = z.mean()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-sYFOSh5urR","colab_type":"text"},"source":["Let's backprop now.\n","\n","Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`"]},{"cell_type":"code","metadata":{"id":"Q8Hcv6JA57Vs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599681416398,"user_tz":-120,"elapsed":3809,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["out.backward()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WM0WhrD58wG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599681416399,"user_tz":-120,"elapsed":3804,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"1447b36c-be0a-4da9-90e9-f0d99941f1ff"},"source":["print(f\"d(out)/dx : \\n{x.grad}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["d(out)/dx : \n","tensor([[4.5000, 4.5000],\n","        [4.5000, 4.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l2VN1ZJY9vOH","colab_type":"text"},"source":["We should have got a matrix of ``4.5``. Let’s call the ``out``\n","*Tensor* “$o$”.\n","We have that $o = \\frac{1}{4}\\sum_i z_i$,\n","$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n","Therefore,\n","$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n","$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n"]},{"cell_type":"markdown","metadata":{"id":"LphxMv-5-yIc","colab_type":"text"},"source":["Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$,\n","then the gradient of $\\vec{y}$ with respect to $\\vec{x}$\n","is a Jacobian matrix:\n","\n","\\begin{align}J=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","Generally speaking, ``torch.autograd`` is an engine for computing\n","vector-Jacobian product. That is, given any vector\n","$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$,\n","compute the product $v^{T}\\cdot J$. If $v$ happens to be\n","the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$,\n","that is,\n","$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$,\n","then by the chain rule, the vector-Jacobian product would be the\n","gradient of $l$ with respect to $\\vec{x}$:\n","\n","\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n","   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n","   \\vdots & \\ddots & \\vdots\\\\\n","   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","   \\end{array}\\right)\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial y_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial y_{m}}\n","   \\end{array}\\right)=\\left(\\begin{array}{c}\n","   \\frac{\\partial l}{\\partial x_{1}}\\\\\n","   \\vdots\\\\\n","   \\frac{\\partial l}{\\partial x_{n}}\n","   \\end{array}\\right)\\end{align}\n","\n","(Note that $v^{T}\\cdot J$ gives a row vector which can be\n","treated as a column vector by taking $J^{T}\\cdot v$.)\n","\n","This characteristic of vector-Jacobian product makes it very\n","convenient to feed external gradients into a model that has\n","non-scalar output."]},{"cell_type":"markdown","metadata":{"id":"7oFZMAfNBmqM","colab_type":"text"},"source":["Let's take a look at another example, in which the result is not a scalar"]},{"cell_type":"code","metadata":{"id":"Sucs4gAM_pld","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599685656871,"user_tz":-120,"elapsed":798,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"89446e4a-f7c9-4e5b-ce71-7f7e3ddaccd6"},"source":["x = torch.ones(3, requires_grad=True)\n","def f(x):\n","  y = x * 2\n","  z = y * y * 2\n","  return z\n","\n","print(f\"f(x): {f(x)}\") # z is a vector, not a scalar"],"execution_count":31,"outputs":[{"output_type":"stream","text":["f(x): tensor([8., 8., 8.], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T8NuIFI9CV3j","colab_type":"text"},"source":["Now in this case `z` is no longer a scalar. `torch.autograd` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:"]},{"cell_type":"code","metadata":{"id":"odPZ9kb9Ahkh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599685805145,"user_tz":-120,"elapsed":848,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"4fe02db7-c1ef-48a0-8652-60f5b947878e"},"source":["v = torch.tensor([1, 2, 3], dtype=torch.float)\n","f(x).backward(v, retain_graph=True)\n","# torch.autograd.backward(z, grad_tensors=v, retain_graph=True)\n","\n","print(f\"gradint of x: {x.grad}\")"],"execution_count":32,"outputs":[{"output_type":"stream","text":["gradint of x: tensor([16., 32., 48.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m3ludBJKoHRI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599686207966,"user_tz":-120,"elapsed":812,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}},"outputId":"dc656544-0a5b-43ec-b7b4-eb35bd82154b"},"source":["x.grad.zero_() # clear previous gradient otherwise it will get accumulated\n","unit_tensor = torch.ones(3, dtype=torch.float)\n","f(x).backward(unit_tensor, retain_graph=True)\n","print(f\"gradint of x: {x.grad}\")"],"execution_count":33,"outputs":[{"output_type":"stream","text":["gradint of x: tensor([16., 16., 16.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GUJ7J2ProjFX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DS9efPzARicy","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599681416400,"user_tz":-120,"elapsed":3778,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["y.grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7GqERO-A_Kw","colab_type":"text"},"source":["You can also stop autograd from tracking history on Tensors\n","with ``.requires_grad=True`` either by wrapping the code block in\n","``with torch.no_grad():``"]},{"cell_type":"code","metadata":{"id":"OOQQ7S4BClZm","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599681416400,"user_tz":-120,"elapsed":3772,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["print(x.requires_grad)\n","print((x ** 2).requires_grad)\n","\n","with torch.no_grad():\n","\tprint((x ** 2).requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fq-f2Vv4CsYt","colab_type":"text"},"source":["Or by using ``.detach()`` to get a new Tensor with the same\n","content but that does not require gradients:\n","\n"]},{"cell_type":"code","metadata":{"id":"P2t3uuHACu2w","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599681416401,"user_tz":-120,"elapsed":3769,"user":{"displayName":"Tan Haobin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqV6KULf9q-EJYsjzgBomalLLjDtwO3xgi1rr1=s64","userId":"06394409417630818999"}}},"source":["print(x.requires_grad)\n","y = x.detach()\n","print(y.requires_grad)\n","print(x.eq(y).all())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krY7IzuTeYIl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}